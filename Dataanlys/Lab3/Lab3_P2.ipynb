{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise 3: Eigenvalues and SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Low rank approximation and dimensionality reduction with SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many important applications in data analysis where SVD is used. One such application has to do with dimensionality reduction. If we have a big data set with many variables (stored as a big matrix), it may be reasonable to think that not all of the variables are equally important for the problem, and that it can therefore be possible to reduce the dimensionality of the problem. To do this, we can use the SVD. Here, we will look at one such example, namely image compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here go through dimensionality reduction on a stinkbug image, step by step, using SVD. This is one way of compressing images.\n",
    "\n",
    "_1.1) First, import the image by running the cell below. Make sure that you have saved the image in the working directory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('stinkbug.png')\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.2) Unfortunately, the image is stored as a 3D-array, as you can see if you check the shape of ${\\tt img}$ with ``np.shape()``. Check the shape of the image._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason it is a 3D-array is due to the way images are normally stored. It is possible to do SVD on 3D-matrices, but it is not part of this course. To make it simpler we will therefore work with a 2D-array.<br>\n",
    "\n",
    "_1.3) Convert the 3D-image to a 2D-array by running the code snippet below. This will create a new image (${\\tt GrayImage}$) stored in a 2D-array._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = img[:, :, 0]\n",
    "G = img[:, :, 1]\n",
    "B = img[:, :, 2]\n",
    "GrayImage = R * 0.2126 + G * 0.7152 + B * 0.0722"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.4) Check the shape of ${\\tt GrayImage}$, and plot the image again._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Compression using SVD</b><br>\n",
    "_1.5) We will now use SVD to compress the image, which is equal to a dimensionality reduction. We will transform the image-matrix to matrices $U, S$ and $V$, such that $A = U\\cdot S \\cdot V^T$. The idea is to \"cut off\" some of the singular values, and then go back to the image again by multiplying the \"cut-off\" matrices (this is described in more detail below). Run the cell below to perform the SVD._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(GrayImage, full_matrices = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the theory, if $A$ is $m \\times n$, then:<br>\n",
    "- $U$, should be $m \\times m$\n",
    "- $S$, should be $m \\times n$\n",
    "- $V^T$, should be $n \\times n$\n",
    "\n",
    "_1.6) Check the shapes of $U$, $S$ and $V^T$. Are the shapes correct?_<br>\n",
    "\n",
    "Hint: What is $m$ and $n$ in this case (the shape of $A$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, $S$ is not a matrix, but a vector. The reason is that only the singular values, no zeros, are stored.<br>\n",
    "The size of matrix $V$ is also wrong. The reason is that we put `full_matrices=False` in the SVD-call, which generates the so called \"reduced\" or \"economical\" form.\n",
    "<br><br>\n",
    "\n",
    "_1.7) SVD always generates the singular values in descending order. The smaller values contribute less in the multiplication $USV^T$, i.e. contribute less to the matrix $A$. Check the size of the singular values by plotting them (with 1, 2, 3, ... up to the the number singular values on the x-axis)._\n",
    "\n",
    "Hint: Create an x-axis with the command `x=np.arange(1,len(S)+1)` and then plot the singular values stored in $S$ using the command `plt.plot(x,S)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.8) The singular values are obviously only big in the beginning, and the get smaller very rapidly. Change the plot to only look at the first 10 singular values. You can do this by adjusting the plot-command to `plt.plot(x[:10],S[:10])`. Or just look at singular values from the tenth and up, use `plt.plot(x[10:],S[10:])`.<br>\n",
    "Try it!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cut-off the \"tail\" and recreate the matrix $A_c=U_cS_cV^T_c$, where $U_c, S_c, V^T_c$ are the \"cut-off\" matrices, it shouldn't affect the matrix that much as we just remove small singular values.<br>\n",
    "\n",
    "Let's say we decide to keep the $r$ first singular values. We will the have to keep\n",
    "- the $r$ first columns of $U$\n",
    "- the $r$ first singular values in $S$\n",
    "- the $r$ first rows in $V^T$\n",
    "\n",
    "_1.9) Let $r=20$ and try to create $U_c$, $S_c$ and $V^T_c$. Also check the shape of the new matrices._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now changed the dimension of the image from 375 to $r$.<br><br>\n",
    "\n",
    "_1.10) Create a new image-matrix by multiplying $A_c = U_cS_cV^T_c$. According to the `np.linalg.svd` reference manual, the multiplication can be done according to `np.dot(U * S, Vt)` for the reduced form of SVD. Do the multiplication and create a new image-matrix $A_c$. Then use `plt.imshow(A_c, cmap='gray')` to draw the compressed image. Does it look a lot different from the original image?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.11) Try different degrees of dimensionality reduction by varying the parameter $r$. How many dimensions can you remove if the image should still look acceptable?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check This Out</h3>\n",
    "You can play around with image compression on real colour images here: <a href=\"https://timbaumann.info/svd-image-compression-demo/\" target=\"_blank\">Image compression with SVD demo</a>. You can see the size of the singular values for different images, and also change the compression with a little ruler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another well-known dimensionality reduction technique in data analysis is Principal Component analysis, PCA. The idea is to find the directions where most variation (the covariance) is located. Often it's possible to find two axes where most of the data variation is, and then reduce the dimensionality to two. PCA is also based on the SVD-decomposition.\n",
    "\n",
    "Here, we engage in practice with point clouds in both two and three dimensions. In the cell below, we are creating a 2D random point cloud and plotting it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 2D cloud\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = 0.5*np.random.normal(size = 200)\n",
    "y = 4 * np.random.normal(size = 200)\n",
    "A = np.array([x,y])\n",
    "# rotate the points anticlockwise by pi/4\n",
    "R  = np.array([[np.cos(np.pi/4),np.sin(np.pi/4)],[-np.sin(np.pi/4),np.cos(np.pi/4)]]) \n",
    "A = R@A \n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.scatter(A[0,:],A[1,:],marker = '.',color = 'red')\n",
    "plt.title(\"A 2D cloud\")\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To project these points to the first principal direction (say $u_1$), compute the SVD of $A$, and compute the rank one approximation $A_1 = U_1\\Sigma_1 V_1$. Finally plot $A_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $A$ is a $2\\times 200$ matrix, its rank-2 approximation will effectively yield the original matrix $A$ itself. Now, let's work with a random 3D point cloud generated using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 3D cloud\n",
    "x = 0.5*np.random.normal(size = 500)\n",
    "y = 8*np.random.normal(size = 500)\n",
    "z = 4*np.random.normal(size = 500)\n",
    "A = np.array([x,y,z])\n",
    "c,s = np.cos(np.pi/4),np.sin(np.pi/4)\n",
    "R1  = np.array([[c,s,0],[-s,c,0],[0,0,1]])\n",
    "c,s = np.cos(-np.pi/4),np.sin(-np.pi/4)\n",
    "R2  = np.array([[1,0,0],[0,c,s],[0,-s,c]])\n",
    "c,s = np.cos(np.pi/2),np.sin(np.pi/2)\n",
    "R3  = np.array([[c,0,s],[0,1,0],[-s,0,c]])\n",
    "A = R1@R2@R3@A \n",
    "# plot\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = plt.axes(projection =\"3d\") \n",
    "ax.scatter3D(A[0,:], A[1,:], A[2,:],color = 'red', marker ='.')\n",
    "plt.title(\"A 3D cloud\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the SVD of matrix $A$ and project this point cloud onto the first principal direction $u_1$ (reducing the dimension to 1). Then, project the points onto the plane formed by the first and second principal directions $u_1$ and $u_2$ (reducing the dimension to 2). For each scenario, create a plot illustrating the projected points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
