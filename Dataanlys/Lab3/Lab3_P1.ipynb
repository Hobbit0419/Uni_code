{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise 3: Eigenvalues and SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Eigenvalues and SVD in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This part of the lab is an introduction to eigenvalues, eigenvectors, singular values and SVD in Python. Whereas you probably worked with eigenvalues/eigenvectors before, singular values and SVD is probably new._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues - Short Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $A$ times vector $x$ is a linear transformation of $x$ that stretches and rotates the vector $x$. \n",
    "We will begin by getting some intuition for such transformations using a particular matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.1) Run the cell below to import the required packages and define the function `plot_vect`. This function can be used to plot two vectors in a coordinate system. The next cell defines a matrix $A$ and vector $x$. Run this cell to plot the vector $x$ together with the linear transformation $Ax$ on the specified domain. Change the vector $x$ a few times to convince yourself that $Ax$ results in stretching and rotating of the vector $x$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_vect(x, b, xlim, ylim):\n",
    "    '''\n",
    "    function to plot two vectors, \n",
    "    x - the original vector\n",
    "    b - the transformed vector\n",
    "    xlim - the limit for x\n",
    "    ylim - the limit for y\n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.quiver(0,0,x[0],x[1],\\\n",
    "        color='k',angles='xy',\\\n",
    "        scale_units='xy',scale=1,\\\n",
    "        label='Original vector')\n",
    "    plt.quiver(0,0,b[0],b[1],\\\n",
    "        color='g',angles='xy',\\\n",
    "        scale_units='xy',scale=1,\\\n",
    "        label ='Transformed vector')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 0],[0, -0.5]])\n",
    "x = np.array([[-1],[1]])\n",
    "Ax = A@x\n",
    "plot_vect(x, Ax, (-3,3), (-3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix used in the previous task is\n",
    "$ \n",
    "A = \\left( \\begin{array}{cc}\n",
    "2 & 0 \\\\\n",
    "0 & -0.5\n",
    "\\end{array} \\right)\n",
    "$\n",
    "with eigenvectors \n",
    "$\n",
    "\\begin{array}{ll}\n",
    "v_1 = \\left( \\begin{array}{c} \n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right)\n",
    "&\n",
    "v_2 = \\left( \\begin{array}{c} \n",
    "1 \\\\\n",
    "0\n",
    "\\end{array} \\right)\n",
    "\\end{array}.\n",
    "$\n",
    "\n",
    "To remind ourselves what an eigenvector is, we will use the matrix $A$ to perform a linear transformation of these eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.2) Repeat the previous task, but define the eigenvectors of the matrix $A$ as $x$-vector. In what way is the linear transformation different now (compared to the transformation obtained using any other vector)?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion is that eigenvectors are vectors in which the transformation only lead to scaling (no rotation), and the scaling factor is the corresponding eigenvalue. This can be written $Av = \\lambda v$, which is the eigenvalue problem. Thus, the eigenvalue problem for a matrix $A$ is solved by an eigenvalue $\\lambda$, together with the corresponding eigenvector $v$.\n",
    "\n",
    "_1.3) From the plots you obtained in Task 1.2), what are the eigenvalues of $A$?_\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy, the command `S, V = np.linalg.eig(A)` computes the eigenvalues/eigenvectors of the matrix $A$, where $S$ is a vector with A's eigenvalues and $V$ is a matrix containing the corresponding eigenvectors stored as columns. The eigenvectors are always normalized (unit length). <br>\n",
    "\n",
    "_2.1) Use `np.linalg.eig` to find the eigenvalues and eigenvectors of the matrix_\n",
    "\n",
    "a) $A =\\left( \\begin{array}{cc}\n",
    " 2 & 0 \\\\\n",
    " 0 & -0.5\n",
    " \\end{array} \\right)$ (the marix used above)\n",
    " <br><br>\n",
    "b) $B =\\left( \\begin{array}{ccc}\n",
    " 2 & 3 & 1 \\\\\n",
    " 0 & 0.5 & 2 \\\\\n",
    " 0 &  0  & 1\n",
    " \\end{array} \\right) $ (Note, the eigenvalues of a triangular matrix are equal to the diagonal elements)\n",
    " <br><br>\n",
    "c) $C =\\left( \\begin{array}{ccc}\n",
    " -1 & 3 & 1 \\\\\n",
    " 1 & 0.5 & 2 \\\\\n",
    " 1 &  -4  & 5\n",
    " \\end{array} \\right) $ (Note, complex eigenvalues always comes in complex conjugate pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to _the spectral theorem_, a symmetric $n\\times n$-matrix $A$ always has $n$ distinct and real eigenvalues and the associated eigenvectors are orthogonal. Thus, for an eigenvalue $\\lambda_i$ of a symmetric matrix $A$ with corresponding eigenvector $v_i$, it holds that $Av_i=\\lambda_i v_i$ for each $i$, so that $Av_1=\\lambda_1 v_1, \\ldots , Av_n = \\lambda_n v_n$, where the $v_i's$ are orthogonal. If we put $v_i$ into a matrix we can write <br>\n",
    "$$\n",
    "A \\left( \\begin{array}{ccc} \n",
    "  & & \\\\\n",
    " v_1& \\cdots & v_n \\\\\n",
    "  & & \n",
    "  \\end{array} \\right) = \\left( \\begin{array}{ccc} \n",
    "  & & \\\\\n",
    " v_1& \\cdots & v_n \\\\\n",
    "  & & \n",
    "  \\end{array} \\right)\n",
    "  \\left( \\begin{array}{ccc} \n",
    "  \\lambda_1& & \\\\\n",
    "   & \\ddots &  \\\\\n",
    "  & & \\lambda_n \n",
    "  \\end{array} \\right) \\Rightarrow AV = V D.\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Thus, since the eigenvectors are orthogonal, it holds that $V^TV = VV^T = I$, where $I$ is the identity matrix. The matrix $V$ is said to be orthogonal.\n",
    "\n",
    "_2.2) Check that this is true for a couple of symmetric matrices, i.e. check that the matrix has $n$ distrinct eigenvalues and that the corresponding eigenvectors are orthogonal using the method described above. You can for example use the matrices: \n",
    " <br><br>\n",
    "a) $A =\\left( \\begin{array}{ccc}\n",
    " 2 & 3 & 1 \\\\\n",
    " 3 & 0.5 & 2 \\\\\n",
    " 1 &  2  & 1\n",
    " \\end{array} \\right) $ <br><br>\n",
    "b) $C = B^TB$, where $B =\\left( \\begin{array}{ccc}\n",
    " -1 & 3 & 1 \\\\\n",
    " 1 & 0.5 & 2 \\\\\n",
    " 1 &  -4  & 5\n",
    " \\end{array} \\right) $._ <br>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The relations you have seen above, $AV = V D$ together with $V^TV = VV^T = I$, lead to an important result, namely <i>the eigendecomposition</i> of $A$: $A = V D V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The singular value decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors require square matrices. What can be done when the matrix $A$ is non-square, for example an over-determined system (more rows than columns)?<br>\n",
    "We can work with $A^TA$ and $AA^T$ instead. These are always symmetric and we can therefore find the eigendemposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_3.1) Define the matrix_\n",
    "$A =\\left( \\begin{array}{ccc}\n",
    " 2 & 3 & 1 \\\\\n",
    " 3 & 0.5 & 2 \\\\\n",
    " 1 &  2  & 1 \\\\\n",
    " 0 & 1 & 2 \n",
    " \\end{array} \\right) $\n",
    " _and compute the eigenvalues and eigenvectors of $A^TA$ and $AA^T$, respectively. Compare the eigenvalues of the two matrices. Are they similar in any way?_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have noted that $A^TA$ and $AA^T$ have the same non-zero eigenvalues (they might come in different order, though), and the remaining eigenvalues are zero. This is always the case. Now, we will compare the results with a decomposition called the _Singular Value Decomposition (SVD)_. The SVD of a matrix $A$ is defined as $A=U \\Sigma V^T$, where $U$ is a matrix containing the so-called left singular vectors of $A$ in its columns, $V$ is a matrix containing the right singular vectors of $A$ in its columns, and $\\Sigma$ is a diagonal matrix defined as below, containing the singular values of $A$, $\\sigma_i$, along its diagonal:\n",
    "$$\n",
    "\\Sigma = \\left( \\begin{array}{ccc} \\sigma_1 & & \\\\ & \\ddots & \\\\ &  & \\sigma_n \\end{array} \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_3.2) What does the SVD of $A$ have to do with the matrices $A^TA$ and $AA^T$? The code snippet below calculates the SVD of the matrix $A$ (the same matrix $A$ as in Task 3.1). Run the cell and compare the SVD with eigenvalues and eigenvectors of $A^TA$ and $AA^T$ you found in the previous task. What similarities do you find?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(A, full_matrices=True)   # The SVD of A, singular values in S\n",
    "V = np.transpose(Vt)\n",
    "\n",
    "print(f'The singular values of A are: {S}')\n",
    "print('Hint: Look at the square of the singular values!')\n",
    "\n",
    "print('The matrix U:')\n",
    "print(U)\n",
    "\n",
    "print('The matrix V:')\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** <br>\n",
    "$A$'s left singular vectors $U$ (columns of U) are equal to the eigenvectors of $AA^T$ (sign might differ)<br>\n",
    "$A$'s right singular vectors $V$ (columns of V) are equal to the eigenvectors of $A^TA$ (sign might differ)<br>\n",
    "$A$'s singular values are the square root of $A^TA$'s eigenvalues (or $AA^T$'s nonzero eigenvalues) <br>\n",
    "<br>\n",
    "There are many applications where SVD is used (you will see one in the next part of this lab). In data analysis, we often have big non-square matrices with data, and SVD is as an important tool. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
