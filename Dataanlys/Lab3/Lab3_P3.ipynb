{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise 3: Eigenvalues and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Computation of eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part of the lab, we learned how to calculate the eigenvalues and eigenvectors of a matrix in Python. But what methods are used under the hood? In this part we look at two numerical methods used for eigenvalue computations. From mathematics, you know that eigenvalues can be computed via the characteristic equation  ùëëùëíùë°(ùê¥‚àíùúÜùêº)=0 . This is not an option in software though, since the coefficients of the characteristic equation can't be computed from determinant evaluations in a numerically stable way. It leads to large perturbations in the roots. In this lab, we will have a look at some computational methods to calculate the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest numerical method for eigenvalue computations is the Power Method. We outline the algorithm below:<br>\n",
    "<br>\n",
    "Given a matrix $A$,\n",
    "- Start with an initial eigenvector $v^{(0)}$, which is a guess, input by the user\n",
    "- Calculate $v^{(1)}=Av^{(0)}, v^{(2)} = Av^{(1)}, v^{(3)} = Av^{(2)}, \\ldots$, and so on. The sequence of $v^{(i)}$'s are normalized throughout the process\n",
    "\n",
    "Using this calculation process, it can be shown that $v^{(i)}$ approaches one eigenvector of the matrix $A$. We terminate the process when we have reached a vector $v^{(i)}$ that is close enough to the true eigenvector (when the difference between two consecutive $v^{(i)}$ is smaller than a given tolerance). When the eigenvector is found, the eigenvalue can be computed with the formula $\\lambda=\\frac{v^TAv}{v^Tv}$ (the denominator disappears when $v$ is normalized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the Power Method yourself following the instructions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_1.1) First, define the matrix_ \n",
    "$A = \\left( \\begin{array}{ccc} \n",
    "1 & 2 & 0 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "1 & 3 & 1\n",
    "\\end{array} \\right) $\n",
    "_and check eigenvalues/eigenvectors using numpy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1.2) Now implement the Power Method (with a for-loop so you can choose number of iterations). You can find a pseudocode for the implementation below. Make sure you understand how the pseudocode corresponds to the algorithm presented above, and then use it to write an actual Python implementation._\n",
    "```\n",
    "  v0 = ...  # initial eigenvector (initial guess), you can use v0=(1, 1,...,1)\n",
    "  Normalize v0\n",
    "  N = ...   # Number of iterations\n",
    "  for k in range(N):\n",
    "     v = Av0\n",
    "     v = v/norm(v)  # Normalize v\n",
    "     e = v.T(Av)    # Compute eigenvalue\n",
    "     v0 = v\n",
    "``` \n",
    "_1.3) When your code works, run it for different number of iterations ${\\tt N}$. You can for example start with 5 iterations and then increase until you have for example 6 correct decimal places (compared with the true eigenvalues calculated above)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, answer the questions\n",
    "- Which eigenvalue (and corresponding eigenvector) does the Power Method find?\n",
    "- Is it exactly the same eigenvector compared with `numpy.linalg.eig`?\n",
    "\n",
    "<br>\n",
    "The Power method is an example of an <i>iterative method</i>, meaning that we get a sequence of solutions $\\{\\lambda_0, \\lambda_1, \\ldots \\}$ (one solution each iteration), and that the sequence approaches (converges to) the true solution.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The QR-iteration method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious problem with the Power Method is that we can only find one eigenvalue and one eigenvector at a time. The most common algorithm for computing eigenvalues and eigenvectors is the <i>QR-iteration</i> outlined below:\n",
    "- Start with finding the QR-decomposition: $A=QR$\n",
    "- Flip the order around and calculate a new matrix: $A_1= RQ$\n",
    "- Find the QR-decomposition of the new matrix: $A_1=Q_1R_1$\n",
    "- Flip the order again: $A_2=R_1Q_1$\n",
    "- And so forth until result is good enough\n",
    "\n",
    "Almost magically, the eigenvalues can be found on the main diagonal in the matrix $A_k$ obtained at the $k$th iteration. As with the Power Method, this is also an iterative method which converges to the true solution after a sufficient number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_2.1) Implement the QR-iteration using a for-loop (based on the outline above). Then, run the algorithm for different number of iterations $N$. Compare the diagonal entries of the resulting matrix $A_N$ with the eigenvalues you computed in Task 1.1). Does the QR-iteration find all eigenvalues of $A$?_ \n",
    "\n",
    "Note that the matrix approaches an upper triangular matrix (the elements under the main diagonal gets smaller as you increase the number of iterations). Remember that the eigenvalues of a triangular matrix can be found on the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_2.2) Below is the the QR-iteration defined as a function (starting from the input matrix A, and running until convergence). Call the function with_\n",
    "- The same matrix $A$ as before\n",
    "- A symmetric matrix, for example $B=A^TA$. Compare the result with the Spectral Theorem, i.e. the resulting matrix should be roughly \n",
    "$$\n",
    "\\Lambda= \\left( \\begin{array}{ccc} \n",
    "\\lambda_1 &  & 0 \\\\\n",
    "          & \\ddots & \\\\\n",
    "        0  &        & \\lambda_n\n",
    "          \\end{array} \\right)\n",
    "          $$\n",
    " \n",
    "**Caution:** The iterative process involves estimating the error at each iteration by comparing the diagonal elements of $A_k$ (approximated eigenvalues) with those of $A_{k-1}$. The iteration terminates if the maximum error falls below a specified tolerance (`tol`). It is crucial to note that this criterion is applicable only when all eigenvalues of $A$ are real (upper triangular Schur form), such as in the case of symmetric matrices. For complex eigenvalues this criterion may not provide reliable convergence assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QRiteration(A0, tol=1.0e-8):\n",
    "    # Eigenvalue computation, QR-iteration\n",
    "    # Default error tolerance in max-norm is 1.0e-8 \n",
    "\n",
    "    if tol<1.0e-15:   # Tol cant be smaller than machine epilon\n",
    "        tol = 1.0e-14\n",
    "    \n",
    "    err = 1.0\n",
    "    while err > tol:\n",
    "        Q1, R1 = np.linalg.qr(A0)\n",
    "        A1 = R1@Q1\n",
    "        A1diag = np.diag(A1)   # Get A1s main diagonal \n",
    "        A0diag = np.diag(A0)   # Get A0s main diagonal\n",
    "        err = (np.linalg.norm(A1diag-A0diag,np.inf))/np.linalg.norm(A1diag,np.inf) # rel. error in max-norm\n",
    "        A0 = A1\n",
    "    \n",
    "    return A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some notes on the QR-method we have implemented today:**\n",
    "\n",
    "- The QR-iteration method above only computes the eigenvalues, but it is possible to also the get the eigenvectors (we will not talk about this today, however). <br>\n",
    "- The QR-iteration method is very expensive due the the QR-decomposition in each iteration. In the standard method the matrix can be transformed to a **Hessenberg** form before the loop, and that reduces the cost significantly. Also the convergence rate can be improved (leading to fewer iteration). This makes the practical QR-method efficient.<br>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
